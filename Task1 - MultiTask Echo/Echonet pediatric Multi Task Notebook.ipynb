{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10824324,"sourceType":"datasetVersion","datasetId":6721127}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-23T15:04:25.028891Z","iopub.execute_input":"2025-02-23T15:04:25.029133Z","iopub.status.idle":"2025-02-23T15:04:28.462567Z","shell.execute_reply.started":"2025-02-23T15:04:25.029112Z","shell.execute_reply":"2025-02-23T15:04:28.461358Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimport collections\nimport pandas\n# import intel_extension_for_pytorch as ipex\n\nimport numpy as np\nimport skimage.draw\nimport pathlib\nimport torchvision\n\nfile_names = []\nclass Echo(torchvision.datasets.VisionDataset):\n    \n\n    def __init__(self, root=None,\n                 split=\"train\", target_type=\"EF\",\n                 mean=0., std=1.,\n                 length=16, period=2,\n                 max_length=250,\n                 clips=1,\n                 pad=None,\n                 rotate=None,\n                 noise=None,\n                 target_transform=None,\n                 external_test_location=None):\n        super().__init__(root, target_transform=target_transform)\n\n        super(Echo, self).__init__(root, target_transform=target_transform)\n\n        if root is None:\n            root = \"/content/EchoNet-Dynamic\"\n\n        self.root = pathlib.Path(root)\n        self.split = split.upper()\n        if not isinstance(target_type, list):\n            target_type = [target_type]\n        self.target_type = target_type\n        self.mean = mean\n        self.std = std\n        self.length = length\n        self.max_length = max_length\n        self.period = period\n        self.clips = clips\n        self.pad = pad\n        self.rotate = rotate\n        self.noise = noise\n        self.target_transform = target_transform\n        self.external_test_location = external_test_location\n\n        self.fnames, self.outcome = [], []\n\n        if self.split == \"EXTERNAL_TEST\":\n            self.fnames = sorted(os.listdir(self.external_test_location))\n        else:\n            # Load video-level labels\n            with open(self.root / \"FileList_new.csv\") as f:\n                data = pandas.read_csv(f)\n            data[\"Split\"].map(lambda x: x.upper())\n\n            if self.split != \"ALL\":\n                data = data[data[\"Split\"] == self.split]\n\n            self.header = data.columns.tolist()\n            self.fnames = data[\"FileName\"].tolist()\n            self.fnames = [fn + \".avi\" for fn in self.fnames if os.path.splitext(fn)[1] == \"\"]  # Assume avi if no suffix\n            print(\"videos way before: \",len(self.fnames))\n            self.outcome = data.values.tolist()\n            \n#             # Check that files are present\n            missing = set(self.fnames) - set(os.listdir(os.path.join(self.root, \"Videos\")))\n            if len(missing) != 0:\n                print(\"{} videos could not be found in {}:\".format(len(missing), os.path.join(self.root, \"Videos\")))\n                for f in sorted(missing):\n                    print(\"\\t\", f)\n                raise FileNotFoundError(os.path.join(self.root, \"Videos\", sorted(missing)[0]))\n\n            # Load traces\n            self.frames = collections.defaultdict(list)\n            self.trace = collections.defaultdict(_defaultdict_of_lists)\n\n            with open(self.root / \"VolumeTracings_new.csv\") as f:\n                header = f.readline().strip().split(\",\")\n                if header == [\"FileName\", \"X1\", \"Y1\", \"X2\", \"Y2\", \"Frame\"]:\n                    for line in f:\n                        filename, x1, y1, x2, y2, frame = line.strip().split(',')\n                        x1 = float(x1)\n                        y1 = float(y1)\n                        x2 = float(x2)\n                        y2 = float(y2)\n                        frame = int(frame)\n                        if frame not in self.trace[filename]:\n                            self.frames[filename].append(frame)\n                        self.trace[filename][frame].append((x1, y1, x2, y2))\n                if header == [\"FileName\", \"X\", \"Y\", \"Frame\"]:\n                    # TODO: probably could merge\n                    for line in f:\n                        values = line.strip().split(',')\n                        # if len(values) != 4 or any(v.strip() == '' for v in values):\n                        #     print(f\"Skipping invalid line: {line.strip()}\")\n                        #     continue\n                        filename, x, y, frame = values\n                        x = float(x)\n                        y = float(y)\n                        frame = int(frame)\n                        # filename, x, y, frame = line.strip().split(',')\n                        # x = float(x)\n                        # y = float(y)\n                        # frame = int(frame)\n                        if frame not in self.trace[filename]:\n                            self.frames[filename].append(frame)\n                        self.trace[filename][frame].append((x, y))\n            for filename in self.frames:\n                for frame in self.frames[filename]:\n                    self.trace[filename][frame] = np.array(self.trace[filename][frame])\n            print(\"videos before: \",len(self.fnames))\n            file_names.append(self.fnames)\n            file_names.append(self.frames)\n            keep = [len(self.frames[f]) >= 2 for f in self.fnames]\n            self.fnames = [f for (f, k) in zip(self.fnames, keep) if k]\n            print(\"videos : \",len(self.fnames))\n            self.outcome = [f for (f, k) in zip(self.outcome, keep) if k]\n\n    def __getitem__(self, index):\n        # Find filename of video\n        if self.split == \"EXTERNAL_TEST\":\n            video = os.path.join(self.external_test_location, self.fnames[index])\n        elif self.split == \"CLINICAL_TEST\":\n            video = os.path.join(self.root, \"ProcessedStrainStudyA4c\", self.fnames[index])\n        else:\n            video = os.path.join(self.root, \"Videos\", self.fnames[index])\n\n        # Load video into np.array\n        video = loadvideo(video).astype(np.float32)\n\n        # Add simulated noise (black out random pixels)\n        # 0 represents black at this point (video has not been normalized yet)\n        if self.noise is not None:\n            n = video.shape[1] * video.shape[2] * video.shape[3]\n            ind = np.random.choice(n, round(self.noise * n), replace=False)\n            f = ind % video.shape[1]\n            ind //= video.shape[1]\n            i = ind % video.shape[2]\n            ind //= video.shape[2]\n            j = ind\n            video[:, f, i, j] = 0\n\n        # Apply normalization\n        if isinstance(self.mean, (float, int)):\n            video -= self.mean\n        else:\n            video -= self.mean.reshape(3, 1, 1, 1)\n\n        if isinstance(self.std, (float, int)):\n            video /= self.std\n        else:\n            video /= self.std.reshape(3, 1, 1, 1)\n\n        # Set number of frames\n        c, f, h, w = video.shape\n        if self.length is None:\n            # Take as many frames as possible\n            length = f // self.period\n        else:\n            # Take specified number of frames\n            length = self.length\n\n        if self.max_length is not None:\n            # Shorten videos to max_length\n            length = min(length, self.max_length)\n\n        if f < length * self.period:\n            # Pad video with frames filled with zeros if too short\n            # 0 represents the mean color (dark grey), since this is after normalization\n            video = np.concatenate((video, np.zeros((c, length * self.period - f, h, w), video.dtype)), axis=1)\n            c, f, h, w = video.shape  # pylint: disable=E0633\n\n        if self.clips == \"all\":\n            # Take all possible clips of desired length\n            start = np.arange(f - (length - 1) * self.period)\n        else:\n            # Take random clips from video\n            start = np.random.choice(f - (length - 1) * self.period, self.clips)\n\n        # Gather targets\n        target = []\n        for t in self.target_type:\n            key = self.fnames[index]\n            if t == \"Filename\":\n                target.append(self.fnames[index])\n            elif t == \"LargeIndex\":\n                # Traces are sorted by cross-sectional area\n                # Largest (diastolic) frame is last\n                target.append(np.int64(self.frames[key][-1]))\n            elif t == \"SmallIndex\":\n                # Largest (diastolic) frame is first\n                target.append(np.int64(self.frames[key][0]))\n            elif t in [\"LargeFrame\", \"SmallFrame\"]:\n                if t == \"LargeFrame\":\n                    frame = self.frames[key][-1]\n                else:\n                    frame = self.frames[key][0]\n\n                if frame is None or frame >= video.shape[1]:\n                    target.append(np.full((video.shape[0], video.shape[2], video.shape[3]), math.nan, video.dtype))\n                else:\n                    target.append(video[:, frame, :, :])\n            elif t in [\"LargeTrace\", \"SmallTrace\"]:\n                if t == \"LargeTrace\":\n                    frame = self.frames[key][-1]\n                else:\n                    frame = self.frames[key][0]\n                if frame is None or frame >= video.shape[1]:\n                    mask = np.full((video.shape[2], video.shape[3]), math.nan, np.float32)\n                else:\n                    t = self.trace[key][frame]\n\n                    if t.shape[1] == 4:\n                        x1, y1, x2, y2 = t[:, 0], t[:, 1], t[:, 2], t[:, 3]\n                        x = np.concatenate((x1[1:], np.flip(x2[1:])))\n                        y = np.concatenate((y1[1:], np.flip(y2[1:])))\n                    else:\n                        assert t.shape[1] == 2\n                        x, y = t[:, 0], t[:, 1]\n\n                    r, c = skimage.draw.polygon(np.rint(y).astype(np.int64), np.rint(x).astype(np.int64), (video.shape[2], video.shape[3]))\n                    mask = np.zeros((video.shape[2], video.shape[3]), np.float32)\n                    mask[r, c] = 1\n                target.append(mask)\n            else:\n                if self.split == \"CLINICAL_TEST\" or self.split == \"EXTERNAL_TEST\":\n                    target.append(np.float32(0))\n                else:\n                    # target.append(np.float32(self.outcome[index][self.header.index(t)]))  # TODO: is floating necessary\n                    target.append(self.outcome[index][self.header.index(t)])\n\n        if target != []:\n            target = tuple(target) if len(target) > 1 else target[0]\n            if self.target_transform is not None:\n                target = self.target_transform(target)\n\n        # Select clips from video\n        video = tuple(video[:, s + self.period * np.arange(length), :, :] for s in start)\n        if self.clips == 1:\n            video = video[0]\n        else:\n            video = np.stack(video)\n\n        if self.pad is not None:\n            # Add padding of zeros (mean color of videos)\n            # Crop of original size is taken out\n            # (Used as augmentation)\n            c, l, h, w = video.shape\n            temp = np.zeros((c, l, h + 2 * self.pad, w + 2 * self.pad), dtype=video.dtype)\n            temp[:, :, self.pad:-self.pad, self.pad:-self.pad] = video  # pylint: disable=E1130\n            i, j = np.random.randint(0, 2 * self.pad, 2)\n            video = temp[:, :, i:(i + h), j:(j + w)]\n\n        return video, target\n\n    def __len__(self):\n        return len(self.fnames)\n\n    def extra_repr(self) -> str:\n        \"\"\"Additional information to add at end of __repr__.\"\"\"\n        lines = [\"Target type: {target_type}\", \"Split: {split}\"]\n        return '\\n'.join(lines).format(**self.__dict__)\n\n\ndef _defaultdict_of_lists():\n    return collections.defaultdict(list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T16:00:01.050026Z","iopub.execute_input":"2025-02-23T16:00:01.050329Z","iopub.status.idle":"2025-02-23T16:00:01.084147Z","shell.execute_reply.started":"2025-02-23T16:00:01.050306Z","shell.execute_reply":"2025-02-23T16:00:01.083155Z"}},"outputs":[],"execution_count":89},{"cell_type":"code","source":"data_dir = \"/kaggle/input/echonet-pediatric/Dataset/A4C\"\nmean, std = get_mean_and_std(Echo(root=data_dir, split=\"train\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T16:00:04.012198Z","iopub.execute_input":"2025-02-23T16:00:04.012560Z","iopub.status.idle":"2025-02-23T16:00:06.306025Z","shell.execute_reply.started":"2025-02-23T16:00:04.012508Z","shell.execute_reply":"2025-02-23T16:00:06.304868Z"}},"outputs":[{"name":"stdout","text":"videos way before:  2580\nvideos before:  2580\nvideos :  2483\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 16/16 [00:01<00:00,  8.02it/s]\n","output_type":"stream"}],"execution_count":90},{"cell_type":"code","source":"\"\"\"Utility functions for videos, plotting and computing performance metrics.\"\"\"\n\nimport os\nimport typing\n\nimport cv2  # pytype: disable=attribute-error\nimport matplotlib\nimport numpy as np\nimport torch\nimport tqdm\n\n\ndef loadvideo(filename: str) -> np.ndarray:\n    \n\n    if not os.path.exists(filename):\n        raise FileNotFoundError(filename)\n    capture = cv2.VideoCapture(filename)\n\n    frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n    frame_width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    v = np.zeros((frame_count, frame_height, frame_width, 3), np.uint8)\n\n    for count in range(frame_count):\n        ret, frame = capture.read()\n        if not ret:\n            raise ValueError(\"Failed to load frame #{} of {}.\".format(count, filename))\n\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        v[count, :, :] = frame\n\n    v = v.transpose((3, 0, 1, 2))\n\n    return v\n\n\ndef savevideo(filename: str, array: np.ndarray, fps: typing.Union[float, int] = 1):\n    \n\n    c, _, height, width = array.shape\n\n    if c != 3:\n        raise ValueError(\"savevideo expects array of shape (channels=3, frames, height, width), got shape ({})\".format(\", \".join(map(str, array.shape))))\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(filename, fourcc, fps, (width, height))\n\n    for frame in array.transpose((1, 2, 3, 0)):\n        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n        out.write(frame)\n\n\ndef get_mean_and_std(dataset: torch.utils.data.Dataset,\n                     samples: int = 128,\n                     batch_size: int = 8,\n                     num_workers: int = 4):\n    \n\n    if samples is not None and len(dataset) > samples:\n        indices = np.random.choice(len(dataset), samples, replace=False)\n        dataset = torch.utils.data.Subset(dataset, indices)\n    dataloader = torch.utils.data.DataLoader(\n        dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n\n    n = 0  # number of elements taken (should be equal to samples by end of for loop)\n    s1 = 0.  # sum of elements along channels (ends up as np.array of dimension (channels,))\n    s2 = 0.  # sum of squares of elements along channels (ends up as np.array of dimension (channels,))\n    for (x, *_) in tqdm.tqdm(dataloader):\n        x = x.transpose(0, 1).contiguous().view(3, -1)\n        n += x.shape[1]\n        s1 += torch.sum(x, dim=1).numpy()\n        s2 += torch.sum(x ** 2, dim=1).numpy()\n    mean = s1 / n  # type: np.ndarray\n    std = np.sqrt(s2 / n - mean ** 2)  # type: np.ndarray\n\n    mean = mean.astype(np.float32)\n    std = std.astype(np.float32)\n\n    return mean, std\n\n\ndef bootstrap(a, b, func, samples=10000):\n   \n    a = np.array(a)\n    b = np.array(b)\n\n    bootstraps = []\n    for _ in range(samples):\n        ind = np.random.choice(len(a), len(a))\n        bootstraps.append(func(a[ind], b[ind]))\n    bootstraps = sorted(bootstraps)\n\n    return func(a, b), bootstraps[round(0.05 * len(bootstraps))], bootstraps[round(0.95 * len(bootstraps))]\n\n\ndef latexify():\n    \n    params = {'backend': 'pdf',\n              'axes.titlesize': 8,\n              'axes.labelsize': 8,\n              'font.size': 8,\n              'legend.fontsize': 8,\n              'xtick.labelsize': 8,\n              'ytick.labelsize': 8,\n              'font.family': 'DejaVu Serif',\n              'font.serif': 'Computer Modern',\n              }\n    matplotlib.rcParams.update(params)\n\n\ndef dice_similarity_coefficient(inter, union):\n    \"\"\"Computes the dice similarity coefficient.\n\n    Args:\n        inter (iterable): iterable of the intersections\n        union (iterable): iterable of the unions\n    \"\"\"\n    return 2 * sum(inter) / (sum(union) + sum(inter))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T15:33:35.764483Z","iopub.execute_input":"2025-02-23T15:33:35.764833Z","iopub.status.idle":"2025-02-23T15:33:35.778065Z","shell.execute_reply.started":"2025-02-23T15:33:35.764808Z","shell.execute_reply":"2025-02-23T15:33:35.777319Z"}},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":"# R(2+1)D Model : Undone\n \"A Closer Look at Spatiotemporal Convolutions for Action Recognition\"","metadata":{}},{"cell_type":"code","source":"# import math\n\n# import torch.nn as nn\n# from torch.nn.modules.utils import _triple\n\n# class SpatioTemporalConv(nn.Module):\n#     def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True):\n#         super(SpatioTemporalConv, self).__init__()\n#         kernel_size = _triple(kernel_size)\n#         stride = _triple(stride)\n#         padding = _triple(padding)\n#         spatial_kernel_size =  [1, kernel_size[1], kernel_size[2]]\n#         spatial_stride =  [1, stride[1], stride[2]]\n#         spatial_padding =  [0, padding[1], padding[2]]\n#         temporal_kernel_size = [kernel_size[0], 1, 1]\n#         temporal_stride =  [stride[0], 1, 1]\n#         temporal_padding =  [padding[0], 0, 0]\n#         intermed_channels = int(math.floor((kernel_size[0] * kernel_size[1] * kernel_size[2] * in_channels * out_channels)/ \\\n#                             (kernel_size[1]* kernel_size[2] * in_channels + kernel_size[0] * out_channels)))\n#         self.spatial_conv = nn.Conv3d(in_channels, intermed_channels, spatial_kernel_size,\n#                                     stride=spatial_stride, padding=spatial_padding, bias=bias)\n#         self.bn = nn.BatchNorm3d(intermed_channels)\n#         self.relu = nn.ReLU()\n#         self.temporal_conv = nn.Conv3d(intermed_channels, out_channels, temporal_kernel_size, \n#                                     stride=temporal_stride, padding=temporal_padding, bias=bias)\n#     def forward(self, x):\n#         x = self.relu(self.bn(self.spatial_conv(x)))\n#         x = self.temporal_conv(x)\n#         return x\n\n\n# class SpatioTemporalResBlock(nn.Module):\n#     r\"\"\"Single block for the ResNet network. Uses SpatioTemporalConv in \n#         the standard ResNet block layout (conv->batchnorm->ReLU->conv->batchnorm->sum->ReLU)\n        \n#         Args:\n#             in_channels (int): Number of channels in the input tensor.\n#             out_channels (int): Number of channels in the output produced by the block.\n#             kernel_size (int or tuple): Size of the convolving kernels.\n#             downsample (bool, optional): If ``True``, the output size is to be smaller than the input. Default: ``False``\n#         \"\"\"\n#     def __init__(self, in_channels, out_channels, kernel_size, downsample=False):\n#         super(SpatioTemporalResBlock, self).__init__()\n#         self.downsample = downsample\n#         padding = kernel_size//2\n\n#         if self.downsample:\n#             # downsample with stride =2 the input x\n#             self.downsampleconv = SpatioTemporalConv(in_channels, out_channels, 1, stride=2)\n#             self.downsamplebn = nn.BatchNorm3d(out_channels)\n#             self.conv1 = SpatioTemporalConv(in_channels, out_channels, kernel_size, padding=padding, stride=2)\n#         else:\n#             self.conv1 = SpatioTemporalConv(in_channels, out_channels, kernel_size, padding=padding)\n#         self.bn1 = nn.BatchNorm3d(out_channels)\n#         self.relu1 = nn.ReLU()\n#         self.conv2 = SpatioTemporalConv(out_channels, out_channels, kernel_size, padding=padding)\n#         self.bn2 = nn.BatchNorm3d(out_channels)\n#         self.outrelu = nn.ReLU()\n\n#     def forward(self, x):\n#         res = self.relu1(self.bn1(self.conv1(x)))    \n#         res = self.bn2(self.conv2(res))\n\n#         if self.downsample:\n#             x = self.downsamplebn(self.downsampleconv(x))\n\n#         return self.outrelu(x + res)\n\n\n# class SpatioTemporalResLayer(nn.Module):\n#     r\"\"\"Forms a single layer of the ResNet network, with a number of repeating \n#     blocks of same output size stacked on top of each other\n        \n#         Args:\n#             in_channels (int): Number of channels in the input tensor.\n#             out_channels (int): Number of channels in the output produced by the layer.\n#             kernel_size (int or tuple): Size of the convolving kernels.\n#             layer_size (int): Number of blocks to be stacked to form the layer\n#             block_type (Module, optional): Type of block that is to be used to form the layer. Default: SpatioTemporalResBlock. \n#             downsample (bool, optional): If ``True``, the first block in layer will implement downsampling. Default: ``False``\n#         \"\"\"\n\n#     def __init__(self, in_channels, out_channels, kernel_size, layer_size, block_type=SpatioTemporalResBlock, downsample=False):\n        \n#         super(SpatioTemporalResLayer, self).__init__()\n#         self.block1 = block_type(in_channels, out_channels, kernel_size, downsample)\n#         self.blocks = nn.ModuleList([])\n#         for i in range(layer_size - 1):\n#             self.blocks += [block_type(out_channels, out_channels, kernel_size)]\n\n#     def forward(self, x):\n#         x = self.block1(x)\n#         for block in self.blocks:\n#             x = block(x)\n\n#         return x\n\n\n# class R2Plus1DNet(nn.Module):\n#     r\"\"\"Forms the overall ResNet feature extractor by initializng 5 layers, with the number of blocks in \n#     each layer set by layer_sizes, and by performing a global average pool at the end producing a \n#     512-dimensional vector for each element in the batch.\n        \n#         Args:\n#             layer_sizes (tuple): An iterable containing the number of blocks in each layer\n#             block_type (Module, optional): Type of block that is to be used to form the layers. Default: SpatioTemporalResBlock. \n#         \"\"\"\n#     def __init__(self, layer_sizes, block_type=SpatioTemporalResBlock):\n#         super(R2Plus1DNet, self).__init__()\n#         self.conv1 = SpatioTemporalConv(3, 64, [3, 7, 7], stride=[1, 2, 2], padding=[1, 3, 3])\n#         self.conv2 = SpatioTemporalResLayer(64, 64, 3, layer_sizes[0], block_type=block_type)\n#         self.conv3 = SpatioTemporalResLayer(64, 128, 3, layer_sizes[1], block_type=block_type, downsample=True)\n#         self.conv4 = SpatioTemporalResLayer(128, 256, 3, layer_sizes[2], block_type=block_type, downsample=True)\n#         self.conv5 = SpatioTemporalResLayer(256, 512, 3, layer_sizes[3], block_type=block_type, downsample=True)\n#         self.pool = nn.AdaptiveAvgPool3d(1)\n    \n#     def forward(self, x):\n#         x = self.conv1(x)\n#         x = self.conv2(x)\n#         x = self.conv3(x)\n#         x = self.conv4(x)\n#         x = self.conv5(x)\n\n#         x = self.pool(x)\n        \n#         return x.view(-1, 512)\n\n# class R2Plus1DClassifier(nn.Module):\n#     def __init__(self, num_classes, layer_sizes, block_type=SpatioTemporalResBlock):\n#         super(R2Plus1DClassifier, self).__init__()\n\n#         self.res2plus1d = R2Plus1DNet(layer_sizes, block_type)\n#         self.linear = nn.Linear(512, num_classes)\n\n#     def forward(self, x):\n#         x = self.res2plus1d(x)\n#         x = self.linear(x) \n\n#         return x   \n\n# r21d_model = R2Plus1DClassifier(num_classes=1, layer_sizes = [2, 2, 2, 2])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T15:33:57.453963Z","iopub.execute_input":"2025-02-23T15:33:57.454279Z","iopub.status.idle":"2025-02-23T15:33:57.459038Z","shell.execute_reply.started":"2025-02-23T15:33:57.454254Z","shell.execute_reply":"2025-02-23T15:33:57.458260Z"}},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":"# C3D Model : Done\n\"Learning Spatiotemporal Features with 3D Convolutional Networks\"","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch\nimport torch.nn.functional as F\n\n\nclass OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(OutConv, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass DoubleConv3D(nn.Module):\n    \"\"\"(convolution => [BN] => ReLU) * 2 for 3D\"\"\"\n\n    def __init__(self, in_channels, out_channels, mid_channels=None):\n        super().__init__()\n        if not mid_channels:\n            mid_channels = out_channels\n        self.double_conv = nn.Sequential(\n            nn.Conv3d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm3d(mid_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv3d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm3d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\n\nclass Up3D(nn.Module):\n    \"\"\"Upscaling then double conv for 3D\"\"\"\n\n    def __init__(self, in_channels, out_channels, bilinear=True):\n        super().__init__()\n\n        # if bilinear, use normal convolutions to reduce the number of channels\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)\n            self.conv = DoubleConv3D(in_channels, out_channels, in_channels // 2)\n        else:\n            self.up = nn.ConvTranspose3d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n            self.conv = DoubleConv3D(in_channels, out_channels)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n\n        # Compute padding sizes for depth, height, and width\n        diffD = x2.size()[2] - x1.size()[2]  # Depth difference\n        diffH = x2.size()[3] - x1.size()[3]  # Height difference\n        diffW = x2.size()[4] - x1.size()[4]  # Width difference\n\n        x1 = F.pad(x1, [diffW // 2, diffW - diffW // 2,\n                        diffH // 2, diffH - diffH // 2,\n                        diffD // 2, diffD - diffD // 2])\n        x = torch.cat([x2, x1], dim=1)  # Concatenate along channel dimension\n        return self.conv(x)\n\nclass C3D(nn.Module):\n    \"\"\"\n    The C3D network as described in\n    Tran, Du, et al. \"Learning spatiotemporal features with 3d convolutional networks.\"\n    Proceedings of the IEEE international conference on computer vision. 2015.\n    \"\"\"\n\n    def __init__(self, num_classes, input_channel=3):\n        super(C3D, self).__init__()\n\n        self.feature1 = nn.Sequential(\n            nn.Conv3d(input_channel, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n        )\n        self.feature2 = nn.Sequential(\n            nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n        )\n        self.feature3 = nn.Sequential(\n            nn.Conv3d(128, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n            nn.Conv3d(256, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)),\n        )\n        self.feature4 = nn.Sequential(\n            nn.Conv3d(256, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n            nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n        )\n        self.feature5 = nn.Sequential(\n            nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n            nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n            nn.AdaptiveMaxPool3d(output_size=(1, 4, 4))\n        )\n\n        self.fc = nn.Sequential(\n            nn.Linear(8192, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout3d(0.5),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout3d(0.5),\n            nn.Linear(4096, num_classes)\n        )\n        self.up1 = Up3D(in_channels=512, out_channels=256, bilinear=False)\n        self.up2 = Up3D(in_channels=256, out_channels=128, bilinear=False)\n        self.up3 = Up3D(in_channels=128, out_channels=64, bilinear=False)\n\n      \n        self.final_up = nn.Sequential(\n            nn.Upsample(scale_factor=(1, 2, 2), mode='trilinear', align_corners=True),  # Only upscale width & height\n            nn.Conv3d(64, 32, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm3d(32),\n            nn.ReLU(inplace=True)\n        )\n      \n        self.out_seg = OutConv(32, num_classes)\n        self.__init_weight()\n\n    def forward(self, x):\n        x1 = self.feature1(x)\n        x2 = self.feature2(x1)\n        x3 = self.feature3(x2)\n        x4 = self.feature4(x3)\n        x5 = self.feature5(x4)\n        \n        x = self.up1(x4, x3)\n        x = self.up2(x, x2)\n        x = self.up3(x, x1)\n        x = self.final_up(x)\n        x = self.out_seg(x)\n        logits = self.fc(x5.view(-1, 8192))\n        return logits,x\n\n    def __init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm3d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\nc3d_model = C3D(num_classes=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T15:34:00.194154Z","iopub.execute_input":"2025-02-23T15:34:00.194437Z","iopub.status.idle":"2025-02-23T15:34:01.087728Z","shell.execute_reply.started":"2025-02-23T15:34:00.194416Z","shell.execute_reply":"2025-02-23T15:34:01.087028Z"}},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":"# X3D Model : Undone\n\"X3D: Expanding Architectures for Efficient Video Recognition models\"","metadata":{}},{"cell_type":"code","source":"# import math\n# from functools import partial\n\n# import torch\n# import torch.nn as nn\n# import torch.nn.functional as F\n\n\n# class SubBatchNorm3d(nn.Module):\n#     \"\"\" FROM SLOWFAST \"\"\"\n#     def __init__(self, num_splits, **args):\n#         super(SubBatchNorm3d, self).__init__()\n#         self.num_splits = num_splits\n#         self.num_features = args[\"num_features\"]\n#         # Keep only one set of weight and bias.\n#         if args.get(\"affine\", True):\n#             self.affine = True\n#             args[\"affine\"] = False\n#             self.weight = torch.nn.Parameter(torch.ones(self.num_features))\n#             self.bias = torch.nn.Parameter(torch.zeros(self.num_features))\n#         else:\n#             self.affine = False\n#         self.bn = nn.BatchNorm3d(**args)\n#         args[\"num_features\"] = self.num_features * self.num_splits\n#         self.split_bn = nn.BatchNorm3d(**args)\n\n#     def _get_aggregated_mean_std(self, means, stds, n):\n#         mean = means.view(n, -1).sum(0) / n\n#         std = (\n#             stds.view(n, -1).sum(0) / n\n#             + ((means.view(n, -1) - mean) ** 2).view(n, -1).sum(0) / n\n#         )\n#         return mean.detach(), std.detach()\n\n#     def aggregate_stats(self):\n#         \"\"\"Synchronize running_mean, and running_var. Call this before eval.\"\"\"\n#         if self.split_bn.track_running_stats:\n#             (\n#                 self.bn.running_mean.data,\n#                 self.bn.running_var.data,\n#             ) = self._get_aggregated_mean_std(\n#                 self.split_bn.running_mean,\n#                 self.split_bn.running_var,\n#                 self.num_splits,\n#             )\n\n#     def forward(self, x):\n#         if self.training:\n#             n, c, t, h, w = x.shape\n#             x = x.view(n // self.num_splits, c * self.num_splits, t, h, w)\n#             x = self.split_bn(x)\n#             x = x.view(n, c, t, h, w)\n#         else:\n#             x = self.bn(x)\n#         if self.affine:\n#             x = x * self.weight.view((-1, 1, 1, 1))\n#             x = x + self.bias.view((-1, 1, 1, 1))\n#         return x\n\n\n# class Swish(nn.Module):\n#     \"\"\" FROM SLOWFAST \"\"\"\n#     \"\"\"Swish activation function: x * sigmoid(x).\"\"\"\n#     def __init__(self):\n#         super(Swish, self).__init__()\n\n#     def forward(self, x):\n#         return SwishEfficient.apply(x)\n\n\n# class SwishEfficient(torch.autograd.Function):\n#     \"\"\" FROM SLOWFAST \"\"\"\n#     \"\"\"Swish activation function: x * sigmoid(x).\"\"\"\n#     @staticmethod\n#     def forward(ctx, x):\n#         result = x * torch.sigmoid(x)\n#         ctx.save_for_backward(x)\n#         return result\n\n#     @staticmethod\n#     def backward(ctx, grad_output):\n#         x = ctx.saved_variables[0]\n#         sigmoid_x = torch.sigmoid(x)\n#         return grad_output * (sigmoid_x * (1 + x * (1 - sigmoid_x)))\n\n\n# def conv3x3x3(in_planes, out_planes, stride=1):\n#     return nn.Conv3d(in_planes,\n#                      out_planes,\n#                      kernel_size=3,\n#                      stride=(1,stride,stride),\n#                      padding=1,\n#                      bias=False,\n#                      groups=in_planes\n#                      )\n\n\n# def conv1x1x1(in_planes, out_planes, stride=1):\n#     return nn.Conv3d(in_planes,\n#                      out_planes,\n#                      kernel_size=1,\n#                      stride=(1,stride,stride),\n#                      bias=False)\n\n\n# class Bottleneck(nn.Module):\n#     def __init__(self, in_planes, planes, stride=1, downsample=None, index=0, base_bn_splits=8):\n#         super(Bottleneck, self).__init__()\n\n#         self.index = index\n#         self.base_bn_splits = base_bn_splits\n#         self.conv1 = conv1x1x1(in_planes, planes[0])\n#         self.bn1 = SubBatchNorm3d(num_splits=self.base_bn_splits, num_features=planes[0], affine=True) #nn.BatchNorm3d(planes[0])\n#         self.conv2 = conv3x3x3(planes[0], planes[0], stride)\n#         self.bn2 = SubBatchNorm3d(num_splits=self.base_bn_splits, num_features=planes[0], affine=True) #nn.BatchNorm3d(planes[0])\n#         self.conv3 = conv1x1x1(planes[0], planes[1])\n#         self.bn3 = SubBatchNorm3d(num_splits=self.base_bn_splits, num_features=planes[1], affine=True) #nn.BatchNorm3d(planes[1])\n#         self.swish = Swish() #nn.Hardswish()\n#         self.relu = nn.ReLU(inplace=True)\n#         if self.index % 2 == 0:\n#             width = self.round_width(planes[0])\n#             self.global_pool = nn.AdaptiveAvgPool3d((1,1,1))\n#             self.fc1 = nn.Conv3d(planes[0], width, kernel_size=1, stride=1)\n#             self.fc2 = nn.Conv3d(width, planes[0], kernel_size=1, stride=1)\n#             self.sigmoid = nn.Sigmoid()\n#         self.downsample = downsample\n#         self.stride = stride\n\n#     def round_width(self, width, multiplier=0.0625, min_width=8, divisor=8):\n#         if not multiplier:\n#             return width\n\n#         width *= multiplier\n#         min_width = min_width or divisor\n#         width_out = max(\n#             min_width, int(width + divisor / 2) // divisor * divisor\n#         )\n#         if width_out < 0.9 * width:\n#             width_out += divisor\n#         return int(width_out)\n\n\n#     def forward(self, x):\n#         residual = x\n\n#         out = self.conv1(x)\n#         out = self.bn1(out)\n#         out = self.relu(out)\n\n#         out = self.conv2(out)\n#         out = self.bn2(out)\n#         # Squeeze-and-Excitation\n#         if self.index % 2 == 0:\n#             se_w = self.global_pool(out)\n#             se_w = self.fc1(se_w)\n#             se_w = self.relu(se_w)\n#             se_w = self.fc2(se_w)\n#             se_w = self.sigmoid(se_w)\n#             out = out * se_w\n#         out = self.swish(out)\n\n#         out = self.conv3(out)\n#         out = self.bn3(out)\n\n#         if self.downsample is not None:\n#             residual = self.downsample(x)\n\n#         out += residual\n#         out = self.relu(out)\n\n#         return out\n\n\n# class ResNet(nn.Module):\n\n#     def __init__(self,\n#                  block,\n#                  layers,\n#                  block_inplanes,\n#                  n_input_channels=3,\n#                  shortcut_type='B',\n#                  widen_factor=1.0,\n#                  dropout=0.5,\n#                  n_classes=1,\n#                  base_bn_splits=8,\n#                  task='class'):\n#         super(ResNet, self).__init__()\n\n#         block_inplanes = [(int(x * widen_factor),int(y * widen_factor)) for x,y in block_inplanes]\n#         self.index = 0\n#         self.base_bn_splits = base_bn_splits\n#         self.task = task\n\n#         self.in_planes = block_inplanes[0][1]\n\n#         self.conv1_s = nn.Conv3d(n_input_channels,\n#                                self.in_planes,\n#                                kernel_size=(1, 3, 3),\n#                                stride=(1, 2, 2),\n#                                padding=(0, 1, 1),\n#                                bias=False)\n#         self.conv1_t = nn.Conv3d(self.in_planes,\n#                                self.in_planes,\n#                                kernel_size=(5, 1, 1),\n#                                stride=(1, 1, 1),\n#                                padding=(2, 0, 0),\n#                                bias=False,\n#                                groups=self.in_planes)\n#         self.bn1 = SubBatchNorm3d(num_splits=self.base_bn_splits, num_features=self.in_planes, affine=True) #nn.BatchNorm3d(self.in_planes)\n#         self.relu = nn.ReLU(inplace=True)\n#         self.layer1 = self._make_layer(block,\n#                                        block_inplanes[0],\n#                                        layers[0],\n#                                        shortcut_type,\n#                                        stride=2)\n#         self.layer2 = self._make_layer(block,\n#                                        block_inplanes[1],\n#                                        layers[1],\n#                                        shortcut_type,\n#                                        stride=2)\n#         self.layer3 = self._make_layer(block,\n#                                        block_inplanes[2],\n#                                        layers[2],\n#                                        shortcut_type,\n#                                        stride=2)\n#         self.layer4 = self._make_layer(block,\n#                                        block_inplanes[3],\n#                                        layers[3],\n#                                        shortcut_type,\n#                                        stride=2)\n#         self.conv5 = nn.Conv3d(block_inplanes[3][1],\n#                                block_inplanes[3][0],\n#                                kernel_size=(1, 1, 1),\n#                                stride=(1, 1, 1),\n#                                padding=(0, 0, 0),\n#                                bias=False)\n#         self.bn5 = SubBatchNorm3d(num_splits=self.base_bn_splits, num_features=block_inplanes[3][0], affine=True) #nn.BatchNorm3d(block_inplanes[3][0])\n#         if task == 'class':\n#             self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n#         elif task == 'loc':\n#             self.avgpool = nn.AdaptiveAvgPool3d((None, 1, 1))\n#         self.fc1 = nn.Conv3d(block_inplanes[3][0], 2048, bias=False, kernel_size=1, stride=1)\n#         self.fc2 = nn.Linear(2048, n_classes)\n#         self.dropout = nn.Dropout(dropout)\n\n#         for m in self.modules():\n#             if isinstance(m, nn.Conv3d):\n#                 nn.init.kaiming_normal_(m.weight,\n#                                         mode='fan_out',\n#                                         nonlinearity='relu')\n\n#     def _downsample_basic_block(self, x, planes, stride):\n#         out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n#         zero_pads = torch.zeros(out.size(0), planes - out.size(1), out.size(2),\n#                                 out.size(3), out.size(4))\n#         if isinstance(out.data, torch.cuda.FloatTensor):\n#             zero_pads = zero_pads.cuda()\n\n#         out = torch.cat([out.data, zero_pads], dim=1)\n\n#         return out\n\n#     def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n#         downsample = None\n#         if stride != 1 or self.in_planes != planes[1]:\n#             if shortcut_type == 'A':\n#                 downsample = partial(self._downsample_basic_block,\n#                                      planes=planes[1],\n#                                      stride=stride)\n#             else:\n#                 downsample = nn.Sequential(\n#                     conv1x1x1(self.in_planes, planes[1], stride),\n#                     SubBatchNorm3d(num_splits=self.base_bn_splits, num_features=planes[1], affine=True) #nn.BatchNorm3d(planes[1])\n#                     )\n\n#         layers = []\n#         layers.append(\n#             block(in_planes=self.in_planes,\n#                   planes=planes,\n#                   stride=stride,\n#                   downsample=downsample,\n#                   index=self.index,\n#                   base_bn_splits=self.base_bn_splits))\n#         self.in_planes = planes[1]\n#         self.index += 1\n#         for i in range(1, blocks):\n#             layers.append(block(self.in_planes, planes, index=self.index, base_bn_splits=self.base_bn_splits))\n#             self.index += 1\n\n#         self.index = 0\n#         return nn.Sequential(*layers)\n\n\n#     def replace_logits(self, n_classes):\n#         self.fc2 = nn.Linear(2048, n_classes)\n\n\n#     def update_bn_splits_long_cycle(self, long_cycle_bn_scale):\n#         for m in self.modules():\n#             if isinstance(m, SubBatchNorm3d):\n#                 m.num_splits = self.base_bn_splits * long_cycle_bn_scale\n#                 m.split_bn = nn.BatchNorm3d(num_features=m.num_features*m.num_splits, affine=False).to(m.weight.device)\n#         return self.base_bn_splits * long_cycle_bn_scale\n\n\n#     def aggregate_sub_bn_stats(self):\n#         \"\"\"find all SubBN modules and aggregate sub-BN stats.\"\"\"\n#         count = 0\n#         for m in self.modules():\n#             if isinstance(m, SubBatchNorm3d):\n#                 m.aggregate_stats()\n#                 count += 1\n#         return count\n\n\n#     def forward(self, x):\n#         x = self.conv1_s(x)\n#         print(x.shape)\n#         x = self.conv1_t(x)\n#         print(x.shape)\n#         x = self.bn1(x)\n#         print(x.shape)\n#         x = self.relu(x)\n#         print(x.shape)\n\n#         x = self.layer1(x)\n#         print(x.shape)\n#         x = self.layer2(x)\n#         print(x.shape)\n#         x = self.layer3(x)\n#         print(x.shape)\n#         x = self.layer4(x)\n#         print(x.shape)\n\n#         x = self.conv5(x)\n#         print(x.shape)\n#         x = self.bn5(x)\n#         print(x.shape)\n#         x = self.relu(x)\n#         print(x.shape)\n\n#         x = self.avgpool(x)\n#         print(x.shape)\n\n#         x = self.fc1(x)\n#         print(x.shape)\n#         x = self.relu(x)\n#         print(x.shape)\n\n#         if self.task == 'class':\n#             x = x.squeeze(4).squeeze(3).squeeze(2) # B C\n#             x = self.dropout(x)\n#             x = self.fc2(x)\n#         if self.task == 'loc':\n#             x = x.squeeze(4).squeeze(3).permute(0,2,1) # B T C\n#             x = self.dropout(x)\n#             x = self.fc2(x).permute(0,2,1) # B C T\n\n#         return x\n\n\n# def replace_logits(self, n_classes):\n#         self.fc2 = nn.Linear(2048, n_classes)\n\n\n# def get_inplanes(version):\n#     planes = {'S':[(54,24), (108,48), (216,96), (432,192)],\n#               'M':[(54,24), (108,48), (216,96), (432,192)],\n#               'XL':[(72,32), (162,72), (306,136), (630,280)]}\n#     return planes[version]\n\n\n# def get_blocks(version):\n#     blocks = {'S':[3,5,11,7],\n#               'M':[3,5,11,7],\n#               'XL':[5,10,25,15]}\n#     return blocks[version]\n\n\n# def generate_model(x3d_version, **kwargs):\n#     model = ResNet(Bottleneck, get_blocks(x3d_version), get_inplanes(x3d_version), **kwargs)\n#     return model\n\n# x3d_model = generate_model('S')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T15:34:08.768434Z","iopub.execute_input":"2025-02-23T15:34:08.768801Z","iopub.status.idle":"2025-02-23T15:34:08.775904Z","shell.execute_reply.started":"2025-02-23T15:34:08.768761Z","shell.execute_reply":"2025-02-23T15:34:08.775057Z"}},"outputs":[],"execution_count":57},{"cell_type":"markdown","source":"# Training : MultiTask","metadata":{}},{"cell_type":"code","source":"data_dir = \"/kaggle/input/echonet-pediatric/Dataset/A4C\"\nmean, std = get_mean_and_std(Echo(root=data_dir, split=\"train\"))\nframes=32\nkwargs = {\"target_type\": [\"EF\",\"LargeFrame\", \"SmallFrame\", \"LargeTrace\", \"SmallTrace\", \"LargeIndex\", \"SmallIndex\"],\n            \"mean\": mean,\n            \"std\": std,\n            \"length\": 200,\n            \"period\": 1\n            }\n\n\nprint(f'\\n\\nMean : {mean} \\nStandard Deviation : {std}')\n\nprint(\"\\n\\nThis is what I have got the Echonet Dynamic Dataset - Adults Echo : \")\nprint(\"\"\"Mean : [33.66532  33.742973 33.911003] \nStandard Deviation : [50.45345  50.4825   50.614986]\\n\\n\"\"\")\n\ndataset = {}\n\ndataset[\"train\"] = Echo(root=data_dir, split=\"train\", **kwargs)\ndataset[\"val\"] = Echo(root=data_dir, split=\"val\", **kwargs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T16:05:51.234253Z","iopub.execute_input":"2025-02-23T16:05:51.234647Z","iopub.status.idle":"2025-02-23T16:05:54.042870Z","shell.execute_reply.started":"2025-02-23T16:05:51.234612Z","shell.execute_reply":"2025-02-23T16:05:54.041981Z"}},"outputs":[{"name":"stdout","text":"videos way before:  2580\nvideos before:  2580\nvideos :  2483\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 16/16 [00:01<00:00,  8.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n\nMean : [26.098072 24.678114 28.73842 ] \nStandard Deviation : [45.68355  42.283268 47.268494]\n\n\nThis is what I have got the Echonet Dynamic Dataset - Adults Echo : \nMean : [33.66532  33.742973 33.911003] \nStandard Deviation : [50.45345  50.4825   50.614986]\n\n\nvideos way before:  2580\nvideos before:  2580\nvideos :  2483\nvideos way before:  336\nvideos before:  336\nvideos :  326\n","output_type":"stream"}],"execution_count":100},{"cell_type":"code","source":"ds = dataset['val']\ndataloader = torch.utils.data.DataLoader(ds, batch_size=batch_size, num_workers=num_workers, shuffle=True, pin_memory=True, drop_last=False)\nfor i,j in dataloader:\n    print(i.shape)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T16:24:56.167781Z","iopub.execute_input":"2025-02-23T16:24:56.168116Z","iopub.status.idle":"2025-02-23T16:25:01.802205Z","shell.execute_reply.started":"2025-02-23T16:24:56.168089Z","shell.execute_reply":"2025-02-23T16:25:01.801132Z"}},"outputs":[{"name":"stdout","text":"torch.Size([16, 3, 200, 112, 112])\n","output_type":"stream"}],"execution_count":107},{"cell_type":"code","source":"selected_frames = i[torch.arange(16), :, j[-1], :, :]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T16:35:01.106923Z","iopub.execute_input":"2025-02-23T16:35:01.107340Z","iopub.status.idle":"2025-02-23T16:35:01.114838Z","shell.execute_reply.started":"2025-02-23T16:35:01.107299Z","shell.execute_reply":"2025-02-23T16:35:01.114073Z"}},"outputs":[],"execution_count":111},{"cell_type":"code","source":"import math\nmodel = c3d_model\noptim = torch.optim.SGD(model.parameters(), lr=1e-5, momentum=0.9, weight_decay=1e-5)\nlr_step_period = math.inf\nscheduler = torch.optim.lr_scheduler.StepLR(optim, lr_step_period)\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nnum_epochs = 1\nbatch_size = 4\nnum_workers = 4\noutput_seg = os.path.join(\"MultiTask Model\")\nos.makedirs(output_seg, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T16:44:24.701907Z","iopub.execute_input":"2025-02-23T16:44:24.702219Z","iopub.status.idle":"2025-02-23T16:44:24.708648Z","shell.execute_reply.started":"2025-02-23T16:44:24.702196Z","shell.execute_reply":"2025-02-23T16:44:24.707781Z"}},"outputs":[],"execution_count":123},{"cell_type":"code","source":"model = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T16:44:25.059404Z","iopub.execute_input":"2025-02-23T16:44:25.059743Z","iopub.status.idle":"2025-02-23T16:44:25.065930Z","shell.execute_reply.started":"2025-02-23T16:44:25.059719Z","shell.execute_reply":"2025-02-23T16:44:25.065181Z"}},"outputs":[],"execution_count":124},{"cell_type":"code","source":"def collate_fn(x):\n    x, f = zip(*x)\n    i = list(map(lambda t: t.shape[1], x))\n    x = torch.as_tensor(np.swapaxes(np.concatenate(x, 1), 0, 1))\n    return x, f, i","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T15:34:23.501392Z","iopub.execute_input":"2025-02-23T15:34:23.501712Z","iopub.status.idle":"2025-02-23T15:34:23.506026Z","shell.execute_reply.started":"2025-02-23T15:34:23.501688Z","shell.execute_reply":"2025-02-23T15:34:23.505164Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"def seg_run_epoch(model, dataloader, train, optim, device, save_all=False, block_size=25):\n    total_loss = 0.0  # cumulative loss (as a Python scalar)\n    n = 0           # number of samples processed\n    pos = torch.tensor(0.0, device=device)\n    neg = torch.tensor(0.0, device=device)\n    pos_pix = None  # will hold an (H,W) tensor accumulated over batches\n    neg_pix = None\n    s1 = 0.0      # sum of ground truth EF (unused here, but kept for consistency)\n    s2 = 0.0      # sum of ground truth EF squared\n\n    model.train(train)\n\n    # Initialize accumulators for segmentation intersection/union\n    large_inter = torch.tensor(0.0, device=device)\n    large_union = torch.tensor(0.0, device=device)\n    small_inter = torch.tensor(0.0, device=device)\n    small_union = torch.tensor(0.0, device=device)\n    large_inter_list = []\n    large_union_list = []\n    small_inter_list = []\n    small_union_list = []\n    yhat_list = []\n    y_list = []  # left empty since the original code did not add any elements\n\n    with torch.set_grad_enabled(train):\n        with tqdm.tqdm(total=len(dataloader)) as pbar:\n            for batch_idx, (X_input, (outcome, large_frame, small_frame,\n                                        large_trace, small_trace,\n                                        large_index, small_index)) in enumerate(dataloader):\n                # Move inputs and segmentation ground truths to GPU\n                X_input = X_input.to(device)\n                outcome = outcome.to(device)\n                large_trace = large_trace.to(device)\n                small_trace = small_trace.to(device)\n                # Ensure indices are on the proper device (assumed to be 1D LongTensor)\n                large_index = large_index.to(device)\n                small_index = small_index.to(device)\n\n                # Count pixels for the human segmentation (all on GPU)\n                pos += (large_trace == 1).sum()\n                pos += (small_trace == 1).sum()\n                neg += (large_trace == 0).sum()\n                neg += (small_trace == 0).sum()\n\n                # Count pixels for the computer segmentation.\n                # Note: since (trace == 1).sum(dim=0) returns a tensor (e.g. shape (H, W)),\n                # we accumulate it batchwise. (This assumes that the spatial dimensions stay constant.)\n                current_pos_pix = (large_trace == 1).sum(dim=0) + (small_trace == 1).sum(dim=0)\n                current_neg_pix = (large_trace == 0).sum(dim=0) + (small_trace == 0).sum(dim=0)\n                if pos_pix is None:\n                    pos_pix = current_pos_pix\n                    neg_pix = current_neg_pix\n                else:\n                    pos_pix = pos_pix + current_pos_pix\n                    neg_pix = neg_pix + current_neg_pix\n\n                # Accumulate EF statistics (converted to CPU scalar only for s1/s2)\n                s1 += outcome.sum().item()\n                s2 += (outcome ** 2).sum().item()\n\n                # Forward pass: get the main output and segmentation output\n                outputs, seg_output = model(X_input)\n                # Append the EF predictions as a tensor (remains on GPU)\n                yhat_list.append(outputs.view(-1))\n                ef_loss = torch.nn.functional.mse_loss(outputs.view(-1), outcome.float())\n\n                # Use the actual batch size (assumed to match the first dimension)\n                batch_size = X_input.size(0)\n                indices = torch.arange(batch_size, device=device)\n\n                # --- Diastolic (large) frame processing ---\n                # Select the frames using the provided index per sample.\n                large_frame_selected = seg_output[indices, :, large_index, :, :]\n                # Pass these through the model to get segmentation predictions.\n                y_large = model(large_frame_selected)[\"out\"]\n                loss_large = torch.nn.functional.binary_cross_entropy_with_logits(\n                    y_large[:, 0, :, :], large_trace, reduction=\"sum\"\n                )\n                # Compute intersection/union using torch’s logical operators.\n                pred_large = (y_large[:, 0, :, :] > 0).float()\n                trace_large = (large_trace > 0).float()\n                large_inter += (pred_large * trace_large).sum()\n                large_union += ((pred_large + trace_large) > 0).float().sum()\n                # Per-sample intersection/union (flatten spatial dimensions and sum)\n                large_inter_list.append(\n                    (pred_large * trace_large).view(batch_size, -1).sum(dim=1)\n                )\n                large_union_list.append(\n                    ((pred_large + trace_large) > 0).float().view(batch_size, -1).sum(dim=1)\n                )\n\n                # --- Systolic (small) frame processing ---\n                small_frame_selected = seg_output[indices, :, small_index, :, :]\n                y_small = model(small_frame_selected)[\"out\"]\n                loss_small = torch.nn.functional.binary_cross_entropy_with_logits(\n                    y_small[:, 0, :, :], small_trace, reduction=\"sum\"\n                )\n                pred_small = (y_small[:, 0, :, :] > 0).float()\n                trace_small = (small_trace > 0).float()\n                small_inter += (pred_small * trace_small).sum()\n                small_union += ((pred_small + trace_small) > 0).float().sum()\n                small_inter_list.append(\n                    (pred_small * trace_small).view(batch_size, -1).sum(dim=1)\n                )\n                small_union_list.append(\n                    ((pred_small + trace_small) > 0).float().view(batch_size, -1).sum(dim=1)\n                )\n\n                # Combine losses and update parameters if training.\n                loss = (loss_large + loss_small) / 2 + ef_loss\n                if train:\n                    optim.zero_grad()\n                    loss.backward()\n                    optim.step()\n\n                total_loss += loss.item()\n                n += batch_size\n\n                # Compute baseline metrics using GPU tensors then convert to scalars for display.\n                p_val = pos / (pos + neg + 1e-10)\n                # p_pix is an (H,W) tensor; compute its elementwise “entropy” and average.\n                p_pix_val = ((pos_pix + 1) / (pos_pix + neg_pix + 2)).mean()\n                dice_large = 2 * large_inter / (large_union + large_inter + 1e-10)\n                dice_small = 2 * small_inter / (small_union + small_inter + 1e-10)\n\n                info_str = \"{:.4f} (SEG:{:.4f}, EF:{:.4f}) / {:.4f} {:.4f}, {:.4f}, {:.4f}\".format(\n                    total_loss / n / (112 * 112),\n                    loss.item() / batch_size / (112 * 112),\n                    ef_loss.item() / batch_size,\n                    -p_val.item() * math.log(p_val.item() + 1e-10)\n                    - (1 - p_val.item()) * math.log(1 - p_val.item() + 1e-10),\n                    (-p_pix_val * torch.log(p_pix_val)\n                     - (1 - p_pix_val) * torch.log(1 - p_pix_val)).mean().item(),\n                    dice_large.item(),\n                    dice_small.item()\n                )\n                pbar.set_postfix_str(info_str)\n                pbar.update()\n\n    # After processing, concatenate outputs while keeping them on GPU.\n    if not save_all:\n        yhat = torch.cat(yhat_list)\n    else:\n        yhat = yhat_list\n    y_out = torch.cat(y_list) if y_list else torch.tensor([], device=device)\n    large_inter_tensor = torch.cat(large_inter_list) if large_inter_list else torch.tensor([], device=device)\n    large_union_tensor = torch.cat(large_union_list) if large_union_list else torch.tensor([], device=device)\n    small_inter_tensor = torch.cat(small_inter_list) if small_inter_list else torch.tensor([], device=device)\n    small_union_tensor = torch.cat(small_union_list) if small_union_list else torch.tensor([], device=device)\n\n    return (\n        total_loss / n / (112 * 112),\n        large_inter_tensor,\n        large_union_tensor,\n        small_inter_tensor,\n        small_union_tensor,\n        ef_loss,\n        yhat,\n        y_out,\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T16:48:16.513216Z","iopub.execute_input":"2025-02-23T16:48:16.513512Z","iopub.status.idle":"2025-02-23T16:48:16.531386Z","shell.execute_reply.started":"2025-02-23T16:48:16.513491Z","shell.execute_reply":"2025-02-23T16:48:16.530453Z"}},"outputs":[],"execution_count":126},{"cell_type":"code","source":"import time\nrun_test = True\nwith open(os.path.join(output_seg, \"log.csv\"), \"a\") as f:\n    epoch_resume = 0\n    bestLoss = float(\"inf\")\n    try:\n        # Attempt to load checkpoint\n        checkpoint = torch.load(os.path.join(output_seg, \"checkpoint.pt\"))\n        model.load_state_dict(checkpoint['state_dict'])\n        optim.load_state_dict(checkpoint['opt_dict'])\n        scheduler.load_state_dict(checkpoint['scheduler_dict'])\n        epoch_resume = checkpoint[\"epoch\"] + 1\n        bestLoss = checkpoint[\"best_loss\"]\n        f.write(\"Resuming from epoch {}\\n\".format(epoch_resume))\n    except FileNotFoundError:\n        f.write(\"Starting run from scratch\\n\")\n\n    for epoch in range(epoch_resume, num_epochs):\n        print(\"Epoch #{}\".format(epoch), flush=True)\n        for phase in ['train', 'val']:\n            start_time = time.time()\n            for i in range(torch.cuda.device_count()):\n                torch.cuda.reset_peak_memory_stats(i)\n\n            ds = dataset[phase]\n            dataloader = torch.utils.data.DataLoader(\n                ds, batch_size=batch_size, num_workers=num_workers, shuffle=True, pin_memory=(device == \"cuda\"), drop_last=(phase == \"train\"))\n\n            loss, large_inter, large_union, small_inter, small_union, ef_loss, y_hat, y = seg_run_epoch(model, dataloader, phase == \"train\", optim, device)\n            overall_dice = 2 * (large_inter.sum() + small_inter.sum()) / (large_union.sum() + large_inter.sum() + small_union.sum() + small_inter.sum())\n            large_dice = 2 * large_inter.sum() / (large_union.sum() + large_inter.sum())\n            small_dice = 2 * small_inter.sum() / (small_union.sum() + small_inter.sum())\n            f.write(\"{},{},{},{},{},{},{},{},{},{},{},{},{},{}\\n\".format(epoch,\n                                                                phase,\n                                                                loss,\n                                                                overall_dice,\n                                                                large_dice,\n                                                                small_dice,\n                                                                ef_loss,\n                                                                sklearn.metrics.r2_score(y, yhat),\n                                                                time.time() - start_time,\n                                                                large_inter.size,\n                                                                sum(torch.cuda.max_memory_allocated() for i in range(torch.cuda.device_count())),\n                                                                sum(torch.cuda.max_memory_reserved() for i in range(torch.cuda.device_count())),\n                                                                batch_size))\n            f.flush()\n        scheduler.step()\n\n        # Save checkpoint\n        save = {\n            'epoch': epoch,\n            'state_dict': model.state_dict(),\n            'best_loss': bestLoss,\n            'loss': loss,\n            'r2': sklearn.metrics.r2_score(y, yhat),\n            'opt_dict': optim.state_dict(),\n            'scheduler_dict': scheduler.state_dict(),\n        }\n        torch.save(save, os.path.join(output_seg, \"checkpoint.pt\"))\n        if loss < bestLoss:\n            torch.save(save, os.path.join(output_seg, \"best.pt\"))\n            bestLoss = loss\n\n    # Load best weights\n    if num_epochs != 0:\n        checkpoint = torch.load(os.path.join(output_seg, \"best.pt\"))\n        model.load_state_dict(checkpoint['state_dict'])\n        f.write(\"Best validation loss {} from epoch {}\\n\".format(checkpoint[\"loss\"], checkpoint[\"epoch\"]))\n\n    if run_test:\n        # Run on validation and test\n        for split in [\"val\", \"test\"]:\n            dataset = Echo(root=data_dir, split=split, **kwargs)\n            dataloader = torch.utils.data.DataLoader(dataset,\n                                                        batch_size=batch_size, num_workers=num_workers, shuffle=False, pin_memory=(device == \"cuda\"))\n            loss, large_inter, large_union, small_inter, small_union, ef_loss, y_hat, y = seg_run_epoch(model, dataloader, False, None, device)\n\n            overall_dice = 2 * (large_inter + small_inter) / (large_union + large_inter + small_union + small_inter)\n            large_dice = 2 * large_inter / (large_union + large_inter)\n            small_dice = 2 * small_inter / (small_union + small_inter)\n            with open(os.path.join(output_seg, \"{}_dice.csv\".format(split)), \"w\") as g:\n                g.write(\"Filename, Overall, Large, Small, R2, MAE, RMSE\\n\")\n                for (filename, overall, large, small) in zip(dataset.fnames, overall_dice, large_dice, small_dice):\n                    g.write(\"{},{},{},{}\\n\".format(filename, overall, large, small))\n\n            f.write(\"{} dice (overall): {:.4f} ({:.4f} - {:.4f})\\n\".format(split, *bootstrap(np.concatenate((large_inter, small_inter)), np.concatenate((large_union, small_union)), dice_similarity_coefficient)))\n            f.write(\"{} dice (large):   {:.4f} ({:.4f} - {:.4f})\\n\".format(split, *bootstrap(large_inter, large_union, dice_similarity_coefficient)))\n            f.write(\"{} dice (small):   {:.4f} ({:.4f} - {:.4f})\\n\".format(split, *bootstrap(small_inter, small_union, dice_similarity_coefficient)))\n            f.write(\"{} (one clip) R2:   {:.3f} ({:.3f} - {:.3f})\".format(split, *echonet.utils.bootstrap(y, yhat, sklearn.metrics.r2_score)))\n            f.write(\"{} (one clip) MAE:  {:.2f} ({:.2f} - {:.2f})\".format(split, *echonet.utils.bootstrap(y, yhat, sklearn.metrics.mean_absolute_error)))\n            f.write(\"{} (one clip) RMSE: {:.2f} ({:.2f} - {:.2f})\".format(split, *tuple(map(math.sqrt, echonet.utils.bootstrap(y, yhat, sklearn.metrics.mean_squared_error)))))\n\n            f.flush()\n            with open(os.path.join(output, \"{}_predictions.csv\".format(split)), \"w\") as g:\n                for (filename, pred) in zip(ds.fnames, yhat):\n                    for (i, p) in enumerate(pred):\n                        g.write(\"{},{},{:.4f}\\n\".format(filename, i, p))\n            echonet.utils.latexify()\n            yhat = np.array(list(map(lambda x: x.mean(), yhat)))\n\n            # Plot actual and predicted EF\n            fig = plt.figure(figsize=(3, 3))\n            lower = min(y.min(), yhat.min())\n            upper = max(y.max(), yhat.max())\n            plt.scatter(y, yhat, color=\"k\", s=1, edgecolor=None, zorder=2)\n            plt.plot([0, 100], [0, 100], linewidth=1, zorder=3)\n            plt.axis([lower - 3, upper + 3, lower - 3, upper + 3])\n            plt.gca().set_aspect(\"equal\", \"box\")\n            plt.xlabel(\"Actual EF (%)\")\n            plt.ylabel(\"Predicted EF (%)\")\n            plt.xticks([10, 20, 30, 40, 50, 60, 70, 80])\n            plt.yticks([10, 20, 30, 40, 50, 60, 70, 80])\n            plt.grid(color=\"gainsboro\", linestyle=\"--\", linewidth=1, zorder=1)\n            plt.tight_layout()\n            plt.savefig(os.path.join(output, \"{}_scatter.pdf\".format(split)))\n            plt.close(fig)\n\n            # Plot AUROC\n            fig = plt.figure(figsize=(3, 3))\n            plt.plot([0, 1], [0, 1], linewidth=1, color=\"k\", linestyle=\"--\")\n            for thresh in [35, 40, 45, 50]:\n                fpr, tpr, _ = sklearn.metrics.roc_curve(y > thresh, yhat)\n                print(thresh, sklearn.metrics.roc_auc_score(y > thresh, yhat))\n                plt.plot(fpr, tpr)\n\n            plt.axis([-0.01, 1.01, -0.01, 1.01])\n            plt.xlabel(\"False Positive Rate\")\n            plt.ylabel(\"True Positive Rate\")\n            plt.tight_layout()\n            plt.savefig(os.path.join(output, \"{}_roc.pdf\".format(split)))\n            plt.close(fig)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T16:48:21.812963Z","iopub.execute_input":"2025-02-23T16:48:21.813261Z","iopub.status.idle":"2025-02-23T16:48:23.650037Z","shell.execute_reply.started":"2025-02-23T16:48:21.813239Z","shell.execute_reply":"2025-02-23T16:48:23.648693Z"}},"outputs":[{"name":"stdout","text":"Epoch #0\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-127-2e367d4e5061>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(os.path.join(output_seg, \"checkpoint.pt\"))\n  0%|          | 0/620 [00:01<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-127-2e367d4e5061>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m                 ds, batch_size=batch_size, num_workers=num_workers, shuffle=True, pin_memory=(device == \"cuda\"), drop_last=(phase == \"train\"))\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlarge_inter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlarge_union\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmall_inter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmall_union\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mef_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseg_run_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0moverall_dice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlarge_inter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msmall_inter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlarge_union\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlarge_inter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msmall_union\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msmall_inter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mlarge_dice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlarge_inter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlarge_union\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlarge_inter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-126-c48be64851aa>\u001b[0m in \u001b[0;36mseg_run_epoch\u001b[0;34m(model, dataloader, train, optim, device, save_all, block_size)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0;31m# Forward pass: get the main output and segmentation output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0;31m# Append the EF predictions as a tensor (remains on GPU)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0myhat_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-56-0f925c75ee53>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mx3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         return F.max_pool3d(\n\u001b[0m\u001b[1;32m    297\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool3d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    918\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.20 GiB. GPU 0 has a total capacity of 15.89 GiB of which 1.09 GiB is free. Process 2553 has 14.79 GiB memory in use. Of the allocated memory 14.38 GiB is allocated by PyTorch, and 116.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 1.20 GiB. GPU 0 has a total capacity of 15.89 GiB of which 1.09 GiB is free. Process 2553 has 14.79 GiB memory in use. Of the allocated memory 14.38 GiB is allocated by PyTorch, and 116.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":127},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}